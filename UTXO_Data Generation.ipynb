{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deciphering Bitcoin Blockchain Data by Cohort Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QM_wOfoWVtiM",
        "zhwe8Oh1Vv7w",
        "hog7UNQX3TEM",
        "_2I4t_HOWXot",
        "G4qrxW8Y5p9v",
        "3ARnMOA3WrSc",
        "GYqJSseUas4P",
        "0K73s9oMlTqO",
        "Rbzra9b5eot7"
      ],
      "authorship_tag": "ABX9TyPkPh8WlA1bEl0eYhL0wkEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SciEcon/UTXO/blob/main/Deciphering_Bitcoin_Blockchain_Data_by_Cohort_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaHnzFeMVAAk"
      },
      "source": [
        "# Part I: Data Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM_wOfoWVtiM"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN5ZNqf4UgTn"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import decimal\n",
        "from datetime import datetime, date, timedelta, timezone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3FUxcgJVi1N"
      },
      "source": [
        "#Connect to Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7NvYDgcVk7v"
      },
      "source": [
        "#Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n99kwqBV7vn"
      },
      "source": [
        "#Connect to Google BigQuery\n",
        "PROJECT_ID = 'crypto-291811'\n",
        "\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(project=PROJECT_ID, location='US')\n",
        "dataset_ref = client.dataset('UTXO', project=PROJECT_ID)\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "tables = list(client.list_tables(dataset))\n",
        "\n",
        "# Print names of all tables in the dataset\n",
        "for table in tables:  \n",
        "  print(table.table_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhwe8Oh1Vv7w"
      },
      "source": [
        "## Creating a Table for variables of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tFz0Q1cVyR-"
      },
      "source": [
        "#Create joint_all\n",
        "\n",
        "table_id_inputs = \"crypto-291811.UTXO.joint_all\"\n",
        "job_config = bigquery.QueryJobConfig(destination=table_id_inputs)\n",
        "\n",
        "sql = \"\"\"\n",
        "  SELECT\n",
        "    (outputs.value/POW(10,8)) AS UTXO,  \n",
        "    outputs.block_timestamp,\n",
        "    inputs.block_timestamp AS spent_block_timestamp,\n",
        "    #FORMAT_TIMESTAMP(\"%Y-%m-%d\", block_timestamp) AS block_date,\n",
        "    #FORMAT_TIMESTAMP(\"%Y-%m-%d\", spent_block_timestamp) AS spent_block_date,\n",
        "  FROM \n",
        "    `bigquery-public-data.crypto_bitcoin.outputs` AS outputs\n",
        "  LEFT JOIN \n",
        "    `bigquery-public-data.crypto_bitcoin.inputs` AS inputs\n",
        "  ON outputs.transaction_hash=inputs.spent_transaction_hash  \n",
        "  AND outputs.index = inputs.spent_output_index\n",
        "\"\"\"\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job_inputs = client.query(sql, job_config=job_config)  # Make an API request.\n",
        "query_job_inputs.result()  # Wait for the job to complete.\n",
        "\n",
        "\n",
        "print(\"Query results loaded to the table {}\".format(table_id_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hog7UNQX3TEM"
      },
      "source": [
        "## Create partitioned tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPtoUbqf3Ssv"
      },
      "source": [
        "#Partition Table by born date for data after 2012\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "sql = \"\"\"\n",
        "  CREATE TABLE\n",
        "    `crypto-291811.UTXO.joint_all_partitionedbyborn12`\n",
        "  PARTITION BY\n",
        "    DATE(block_timestamp) AS\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `crypto-291811.UTXO.joint_all`\n",
        "  WHERE\n",
        "    block_timestamp > TIMESTAMP('2012-01-01 00:00:00+00')\n",
        "\"\"\"\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job_inputs = client.query(sql, job_config=job_config)  # Make an API request.\n",
        "query_job_inputs.result()  # Wait for the job to complete.\n",
        "\n",
        "print(\"Query results loaded to the table {}\".format(table_id_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udq-ra9y3e6s"
      },
      "source": [
        "#Partition by death date for data after 2012\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "sql = \"\"\"\n",
        "  CREATE TABLE\n",
        "    `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "  PARTITION BY\n",
        "    DATE(spent_block_timestamp) AS\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    `crypto-291811.UTXO.joint_all`\n",
        "  WHERE\n",
        "    (spent_block_timestamp > TIMESTAMP('2012-01-01 00:00:00+00')\n",
        "    OR \n",
        "    spent_block_timestamp IS NULL)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job_inputs = client.query(sql, job_config=job_config)  # Make an API request.\n",
        "query_job_inputs.result()  # Wait for the job to complete.\n",
        "\n",
        "print(\"Query results loaded to the table {}\".format(table_id_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2I4t_HOWXot"
      },
      "source": [
        "# Part II: Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4qrxW8Y5p9v"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEzTYHCp3pe7"
      },
      "source": [
        "def cal(x):\n",
        "    t=np.sign(x-0.999)+np.sign(x-29.999)+np.sign(x-90.999)+np.sign(x-181.999)+np.sign(x-364.999)+np.sign(x-365*2+0.001)+np.sign(x-365*3+0.001)+np.sign(x-365*4+0.001)+np.sign(x-365*5+0.001)+np.sign(x-365*10+0.001)+1\n",
        "    return t\n",
        "\n",
        "def Task1_born(data):\n",
        "    newborn = data['UTXO'].sum()\n",
        "    return(newborn)\n",
        "\n",
        "#Partitioning By Death Date\n",
        "def Task1_dead(data):\n",
        "    dead = data['UTXO'].sum()\n",
        "    return(dead)\n",
        "\n",
        "def Task2(data):\n",
        "    #data['Life_Length'] = data['spent_block_timestamp']- data['block_timestamp']\n",
        "    #data['Life_Length'] = data['Life_Length'].map(lambda x:x.days).apply(float)\n",
        "    sumUTXO = data['UTXO'].sum()\n",
        "    sumLength = (data['UTXO']*data['Life_Length']).sum()\n",
        "    if sumUTXO == 0:\n",
        "        WALE = 0.0\n",
        "    else:\n",
        "        WALE = sumLength/sumUTXO\n",
        "    return(WALE)\n",
        "    \n",
        "def Task3(data):\n",
        "    data['Life_Length'] = data['spent_block_timestamp']- data['block_timestamp']\n",
        "    data['Life_Length'] = data['Life_Length'].map(lambda x:x.days).apply(float)\n",
        "    data['categorical'] = cal(data['Life_Length'])\n",
        "    categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "    result=pd.DataFrame(np.zeros((1, 11)), columns=categories)\n",
        "    for i in categories:  \n",
        "        result.loc[:,i] = data[data['categorical']==i]['UTXO'].sum()  \n",
        "    return result\n",
        "\n",
        "def Task4(data, date):  \n",
        "    categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "    result=pd.DataFrame(np.zeros((1, 11)), columns=categories)\n",
        "    if len(data)!= 0:\n",
        "      data['Age'] = data['block_timestamp'].apply(lambda x: (working_date-x).days)\n",
        "      data['categorical'] = cal(data['Age'])\n",
        "      for i in categories: \n",
        "        result.loc[:,i] = data[data['categorical']==i]['UTXO'].sum()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ARnMOA3WrSc"
      },
      "source": [
        "## Defining the Processing Programs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxBbfG-9XS0n"
      },
      "source": [
        "def STXOprogram(start, end):\n",
        "  duration=pd.date_range(start=start, end=end)\n",
        "  days = np.size(duration)\n",
        "  categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "  Result=pd.DataFrame(np.zeros((days, 11)), columns=categories)\n",
        "  Result['date'] = duration\n",
        "\n",
        "  for i in range(0, days):\n",
        "    start_date = start + timedelta(days=i)\n",
        "    end_date = start_date + timedelta(days=1)\n",
        "    \n",
        "  #Partitioning by Dead Date\n",
        "    query2 = \"\"\"\n",
        "          SELECT \n",
        "            *\n",
        "          FROM \n",
        "            `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "          WHERE\n",
        "            spent_block_timestamp >= TIMESTAMP('\"\"\" + str(start_date) + \"\"\" 00:00:00+00')\n",
        "          AND \n",
        "            spent_block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\"\"\"\n",
        "    query_job2 = client.query(query2)\n",
        "    # Make an API request  to run the query and return a pandas DataFrame\n",
        "    data2 = query_job2.to_dataframe()  \n",
        "    \n",
        "    #Work on Task3\n",
        "    Result.iloc[i,0:11]=list(Task3(data2).iloc[0])\n",
        "    \n",
        "  #Partitioning by Born Date\n",
        "    query1 = \"\"\"\n",
        "          SELECT \n",
        "            *\n",
        "          FROM \n",
        "            `crypto-291811.UTXO.joint_all_partitionedbyborn12`\n",
        "          WHERE\n",
        "            block_timestamp >= TIMESTAMP('\"\"\" + str(start_date) + \"\"\" 00:00:00+00')\n",
        "          AND \n",
        "            block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\"\"\"\n",
        "    query_job1 = client.query(query1)\n",
        "    # Make an API request  to run the query and return a pandas DataFrame\n",
        "    data1 = query_job1.to_dataframe()\n",
        "    \n",
        "    #Work on Task 1 and Task 2\n",
        "    Result.loc[i,'newborn'] = Task1_born(data1)\n",
        "    Result.loc[i,'dead'] = Task1_dead(data2)\n",
        "    Result.loc[i,'WALE'] = Task2(data2)\n",
        "    Result.columns = ['-9', '-7', '-5', '-3', '-1', '1', '3', '5', '7', '9', '11', 'date', 'newborn', 'dead', 'WALE']\n",
        "  return Result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB4oynVzYl07"
      },
      "source": [
        "def UTXOprogram(start, end):\n",
        "  duration=pd.date_range(start=start, end=end)\n",
        "  days = np.size(duration)\n",
        "  categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "  Dist_Alive=pd.DataFrame(np.zeros((days, 10)), columns=categories)\n",
        "  Dist_Alive['date'] = duration\n",
        "  start_date=start+timedelta(days=1) \n",
        "  end_date =end+timedelta(days=1) \n",
        "  # note the trick below, we only keep data whose block_timestamp<end_date, and spent_block_timestamp>start_date\n",
        "  #must be from joint_all\n",
        "  query = \"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "      WHERE\n",
        "        block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\n",
        "      AND \n",
        "        (spent_block_timestamp >= TIMESTAMP('\"\"\" + str(start_date) + \"\"\" 00:00:00+00')\n",
        "        OR \n",
        "        spent_block_timestamp IS NULL)\n",
        "     \"\"\"\n",
        "  query_job = client.query(query)\n",
        "\n",
        "# Make an API request  to run the query and return a pandas DataFrame\n",
        "  data = query_job.to_dataframe()\n",
        "  data['block_timestamp'] = pd.to_datetime(data['block_timestamp'], format='%Y-%m-%d')\n",
        "  data['spent_block_timestamp'] = pd.to_datetime(data['spent_block_timestamp'], format='%Y-%m-%d')\n",
        "  for j in range(0, days):\n",
        "    working_date = pd.to_datetime(start_date + timedelta(days=j), utc=True)   \n",
        "    working_data = data.loc[((data.block_timestamp<working_date) & ((pd.isna(data.spent_block_timestamp) | (data.spent_block_timestamp>=working_date))))].copy()\n",
        "    Dist_Alive.iloc[j,0:10] = list(Task4(working_data, working_date).iloc[0])\n",
        "\n",
        "  return Dist_Alive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGHGy_L0nbe-"
      },
      "source": [
        "def UTXOprogram1(start, end):\n",
        "  duration=pd.date_range(start=start, end=end)\n",
        "  days = np.size(duration)\n",
        "  categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "  Dist_Alive=pd.DataFrame(np.zeros((days, 11)), columns=categories)\n",
        "  Dist_Alive['date'] = duration\n",
        "  start_date=start+timedelta(days=1) \n",
        "  end_date =end+timedelta(days=1) \n",
        "  # note the trick below, we only keep data whose block_timestamp<end_date, and spent_block_timestamp>start_date\n",
        "  #must be from joint_all\n",
        "  query = \"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "      WHERE\n",
        "        block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\n",
        "      AND \n",
        "        spent_block_timestamp >= TIMESTAMP('\"\"\" + str(start_date) + \"\"\" 00:00:00+00')\n",
        "     \"\"\"\n",
        "  query_job = client.query(query)\n",
        "\n",
        "# Make an API request  to run the query and return a pandas DataFrame\n",
        "  data = query_job.to_dataframe()\n",
        "  data['block_timestamp'] = pd.to_datetime(data['block_timestamp'], format='%Y-%m-%d')\n",
        "  data['spent_block_timestamp'] = pd.to_datetime(data['spent_block_timestamp'], format='%Y-%m-%d')\n",
        "  for j in range(0, days):\n",
        "    working_date = pd.to_datetime(start_date + timedelta(days=j), utc=True)   \n",
        "    working_data = data.loc[((data.block_timestamp<working_date) & ((data.spent_block_timestamp>=working_date)))].copy()\n",
        "    Dist_Alive.iloc[j,0:11] = list(Task4(working_data, working_date).iloc[0])\n",
        "\n",
        "  return Dist_Alive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6YEbI2ni1-e"
      },
      "source": [
        "def UTXOprogram2(start, end):\n",
        "  duration=pd.date_range(start=start, end=end)\n",
        "  days = np.size(duration)\n",
        "  categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "  Dist_Alive=pd.DataFrame(np.zeros((days, 11)), columns=categories)\n",
        "  Dist_Alive['date'] = duration\n",
        "  start_date=start+timedelta(days=1) \n",
        "  end_date =end+timedelta(days=1) \n",
        "  # note the trick below, we only keep data whose block_timestamp<end_date, and spent_block_timestamp>start_date\n",
        "  #must be from joint_all\n",
        "  query = \"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "      WHERE\n",
        "        block_timestamp < TIMESTAMP('2018-12-31 00:00:00+00')\n",
        "      AND \n",
        "        spent_block_timestamp IS NULL\n",
        "     \"\"\"\n",
        "  query_job = client.query(query)\n",
        "\n",
        "# Make an API request  to run the query and return a pandas DataFrame\n",
        "  data = query_job.to_dataframe()\n",
        "  data['block_timestamp'] = pd.to_datetime(data['block_timestamp'], format='%Y-%m-%d')\n",
        "  data['spent_block_timestamp'] = pd.to_datetime(data['spent_block_timestamp'], format='%Y-%m-%d')\n",
        "  for j in range(0, days):\n",
        "    working_date = pd.to_datetime(start_date + timedelta(days=j), utc=True)   \n",
        "    working_data = data.loc[(data.block_timestamp<working_date)].copy()\n",
        "    Dist_Alive.iloc[j,0:11] = list(Task4(working_data, working_date).iloc[0])\n",
        "\n",
        "  return Dist_Alive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBu6s-AE4CX"
      },
      "source": [
        "def UTXOprogram3(start, end):\n",
        "  duration=pd.date_range(start=start, end=end)\n",
        "  days = np.size(duration)\n",
        "  categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11]\n",
        "  Dist_Alive=pd.DataFrame(np.zeros((days, 11)), columns=categories)\n",
        "  Dist_Alive['date'] = duration\n",
        "  start_date=start+timedelta(days=1) \n",
        "  end_date =end+timedelta(days=1) \n",
        "  # note the trick below, we only keep data whose block_timestamp<end_date, and spent_block_timestamp>start_date\n",
        "  #must be from joint_all\n",
        "  query = \"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        `crypto-291811.UTXO.joint_all_partitionedbydeath12`\n",
        "      WHERE\n",
        "        block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\n",
        "      AND\n",
        "        block_timestamp > TIMESTAMP('2018-12-31 00:00:00+00')\n",
        "      AND \n",
        "        spent_block_timestamp IS NULL\n",
        "     \"\"\"\n",
        "  query_job = client.query(query)\n",
        "\n",
        "# Make an API request  to run the query and return a pandas DataFrame\n",
        "  data = query_job.to_dataframe()\n",
        "  data['block_timestamp'] = pd.to_datetime(data['block_timestamp'], format='%Y-%m-%d')\n",
        "  data['spent_block_timestamp'] = pd.to_datetime(data['spent_block_timestamp'], format='%Y-%m-%d')\n",
        "  for j in range(0, days):\n",
        "    working_date = pd.to_datetime(start_date + timedelta(days=j), utc=True)   \n",
        "    working_data = data.loc[(data.block_timestamp<working_date)].copy()\n",
        "    Dist_Alive.iloc[j,0:11] = list(Task4(working_data, working_date).iloc[0])\n",
        "\n",
        "  return Dist_Alive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYqJSseUas4P"
      },
      "source": [
        "## Running the Processing Programs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJjlOe4PavLS"
      },
      "source": [
        "for year in range(2012, 2021):\n",
        "  start = date(year,1,1)\n",
        "  end = date(year,12,31)\n",
        "  STXOresult = STXOprogram(start, end)\n",
        "  address = '/content/drive/My Drive/ResultSTXO' + str(year) + '.csv'\n",
        "  STXOresult.to_csv(address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUpzSwzpdPhP"
      },
      "source": [
        "for year in range(2012, 2021):\n",
        "  start = date(year,1,1)\n",
        "  end = date(year,12,31)\n",
        "  UTXOresult = UTXOprogram(start, end)\n",
        "  address = '/content/drive/My Drive/ResultUTXO' + str(year) + '.csv'\n",
        "  STXOresult.to_csv(address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57GNrWpmdasj"
      },
      "source": [
        "Note: this is how we acquire results for all data from 2012 to 2020. In practice, you may adjust the start and end range to acquire results for any time period you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K73s9oMlTqO"
      },
      "source": [
        "## 2009-2011 Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBfJ3_dUlWP9"
      },
      "source": [
        "#STXO\n",
        "end_date = date(2012,1,1)\n",
        "query1 = \"\"\"\n",
        "          SELECT \n",
        "            *\n",
        "          FROM \n",
        "            `crypto-291811.UTXO.joint_all`\n",
        "          WHERE\n",
        "            block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\"\"\"\n",
        "            \n",
        "query_job1 = client.query(query1)\n",
        "\n",
        "    # Make an API request  to run the query and return a pandas DataFrame\n",
        "data = query_job1.to_dataframe()\n",
        "data['UTXO'] = (data['value']/10**8).apply(float)\n",
        "data = data.drop(['value'], axis = 1)\n",
        "\n",
        "start = date(2009,1,3)\n",
        "end = date(2011,12,31)\n",
        "duration=pd.date_range(start=start, end=end)\n",
        "days = np.size(duration)\n",
        "categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9]\n",
        "Result=pd.DataFrame(np.zeros((days, 10)), columns=categories)\n",
        "Result['date'] = duration\n",
        "\n",
        "for i in range(0, days):\n",
        "    working_start_date = pd.to_datetime(start + timedelta(days=i),utc=True)\n",
        "    working_end_date = pd.to_datetime(start +timedelta(days=1)+timedelta(days=i),utc=True)\n",
        "    working_data2 = data[(data.spent_block_timestamp >= working_start_date) & (data.spent_block_timestamp < working_end_date)].copy()\n",
        "    Result.iloc[i,0:10]=list(Task3(working_data2).iloc[0])\n",
        "    working_data1 = data[(data.block_timestamp >= working_start_date) & (data.block_timestamp < working_end_date)].copy()\n",
        "    Result.loc[i,'newborn'] = Task1_born(working_data1)\n",
        "    Result.loc[i,'dead'] = Task1_dead(working_data2)\n",
        "    Result.loc[i,'WALE'] = Task2(working_data2)\n",
        "Result.to_csv('/content/drive/My Drive/ResultSTXO09-11.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMsxe5Qvl6s6"
      },
      "source": [
        "start = date(2009,1,3)\n",
        "end = date(2011,12,31)\n",
        "duration=pd.date_range(start=start, end=end)\n",
        "days = np.size(duration)\n",
        "categories = [-9, -7, -5, -3, -1, 1, 3, 5, 7, 9]\n",
        "Dist_Alive=pd.DataFrame(np.zeros((days, 10)), columns=categories)\n",
        "Dist_Alive['date'] = duration\n",
        "\n",
        "start_date=start+timedelta(days=1) \n",
        "end_date =end+timedelta(days=1) \n",
        "### note the trick below, we only keep data whose block_timestamp<end_date, and spent_block_timestamp>start_date\n",
        "#must be from joint_all\n",
        "query = \"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        `crypto-291811.UTXO.joint_all`\n",
        "      WHERE\n",
        "        block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\n",
        "     \"\"\"\n",
        "query_job = client.query(query)\n",
        "\n",
        "# Make an API request  to run the query and return a pandas DataFrame\n",
        "data = query_job.to_dataframe()\n",
        "data['block_timestamp'] = pd.to_datetime(data['block_timestamp'], format='%Y-%m-%d')\n",
        "data['spent_block_timestamp'] = pd.to_datetime(data['spent_block_timestamp'], format='%Y-%m-%d')\n",
        "\n",
        "for j in range(0, days):\n",
        "    working_date = pd.to_datetime(start_date + timedelta(days=j), utc=True)   \n",
        "    working_data = data.loc[((data.block_timestamp<working_date) & ((pd.isna(data.spent_block_timestamp) | (data.spent_block_timestamp>=working_date))))].copy()\n",
        "    Dist_Alive.iloc[j,0:10] = list(Task4(working_data, working_date).iloc[0])\n",
        "\n",
        "Dist_Alive.to_csv('/content/drive/My Drive/UTXO/DistAlive09-11.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGidf6M2jiDq"
      },
      "source": [
        "## Merge the Results and Export Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0iDpErJjhHN"
      },
      "source": [
        "#Assuming that all necessary data files are ready in Google Drive\n",
        "start = 2012\n",
        "end = 2020\n",
        "currency = \"Bitcoin\"\n",
        "\n",
        "Task123 = pd.read_csv(\"/content/drive/My Drive/UTXO/ResultSTXO09-11.csv\")\n",
        "Dist_Alive = pd.read_csv(\"/content/drive/My Drive/UTXO/ResultUTXO09-11.csv\")\n",
        "\n",
        "for i in range(start, end+1):\n",
        "  file1 = pd.read_csv('/content/drive/My Drive/UTXO/' + currency + \"Result\" + str(i) + \"Task123.csv\")\n",
        "  file2 = pd.read_csv('/content/drive/My Drive/UTXO/' + currency + \"DistAlive\" + str(i) + \".csv\")\n",
        "  Task123 = Task123.append(file1)\n",
        "  Dist_Alive = Dist_Alive.append(file2)\n",
        "\n",
        "Task123 = Task123.reset_index(drop=True).drop(['Unnamed: 0'], axis = 1)\n",
        "Task123 = Task123[['date', 'newborn', 'dead', 'WALE', '-9', '-7', '-5', '-3', '-1', '1', '3', '5', '7', '9', '11']]\n",
        "Dist_Alive = Dist_Alive.reset_index(drop=True).drop(['Unnamed: 0'], axis = 1)\n",
        "Dist_Alive = Dist_Alive[['date', '-9', '-7', '-5', '-3', '-1', '1', '3', '5', '7', '9', '11']]\n",
        "\n",
        "# Export Data\n",
        "name1 = '/content/drive/My Drive/UTXO/' + currency + 'Task123.csv'\n",
        "name2 = '/content/drive/My Drive/UTXO/' + currency + 'Distalive.csv'\n",
        "Task123.to_csv(name1)\n",
        "Dist_Alive.to_csv(name2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
